{"ts":1353360988436,"silentsave":true,"restoring":false,"patch":[[{"diffs":[[1,"#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom astropy.nddata import NDdata\n'''\nThe ccdproc package provides tools for the reduction and \nanalysis of optical data captured with a CCD.   The package\nis built around the CCDData class, which has built into\nit all of the functions to process data.  The CCDData object  \ncontains all the information to describe the 2-D readout\nfrom a single amplifier/detector. \n\nThe CCDData class inherits from the NDData class as its base object\nand the object on which all actions will be performed.  By\ninheriting from the CCD data class, basic array manipulation\nand error handling are already built into the object.\n\nThe CCDData task should be able to properly deal with the\npropogation of errors and propogate bad pixel frames\nthrough each of the tasks.  It should also update the meta\ndata, units, and WCS information as the data are processed\nthrough each step.\n\nThe following functions are required for performing basic CCD correction:\n-creation of variance frame\n-overscan subtraction\n-bias subtraction \n-trimming the data\n-gain correction\n-xtalk correction\n-dark frames correction\n-flat field correction\n-illumination correction\n-fringe correction\n-scattered light correction\n-cosmic ray cleaning\n-distortion correction \n\nIn addition to the CCDData and CCDList class, the ccdproc does \nrequire some additional features in order to properly \nreduce CCD data. The following features are required\nfor basic processing of CCD data:\n-fitting data\n-combining data\n-re-sampling data\n-transforming data\n\nAll actions of ccdproc should be logged and recorded.\n\nMulti-Extension FITS files can be handled by treating \neach extension as a CCDData object and \n\n'''\n\n# ============\n# Base Objects\n# ============\n'''\nCCDData is an object that inherits from NDData class and specifically\ndescribes an object created from the single readout of a CCD.  \n\nUsers should be able to create a CCDData object from scratch, from\nan existing NDData object, or a single extension from a FITS file. \n\nIn the case of the CCDData, the parameter 'uncertainty' will\nbe mapped to variance as that will be more explicitly describing\nthe information that will be kept for the processing of the \n\n'''\ndata=100+10*np.random.random((110,100))\nccddata=CCDData(data=data)\nccddata=CCDData(NDData.NDData)\nccddata=CCDData(pyfits.ImageHDU)\n\n#Setting basic properties of the object\n# ----------------------\nccddata.variance=data**0.5\nccddata.mask=np.ones(110,100)\nccddata.flags=np.zeros(110,100)\nccddata.wcs=None  #when will this be available?\nccddata.meta={}\nccddata.units=u.adu  #is this valid?  \n\n# Functional Requirements\n# ----------------------\n# A number of these different fucntions are convenient functions that\n# just outline the process that is needed.   The important thing is that\n# the process is being logged and that a clear process is being handled \n# by each step to make building a pipeline easy.   Then again, it might\n# not be easy to handle all possible steps which are needed, and the more\n# important steps will be things which aren't already handled by NDData.\n\n#All functions should propogate throught to the variance frame and \n#bad pixel mask\n\n#convenience function based on a given value for \n#the readnoise and gain.   Units should be specified\n#for these values though.\n#Question: Do we have an object that is just a scalar\n#and a unit?  Or a scalar, unit and an error?   ie, This \n#could actually be handled by the gain and readnoise being\n#specified as an NDData object   \nccddata.createvariance(gain=1.0, readnoise=5.0)\n\n#Overscan subtract the data\n#Should be able to provide the meta data for\n#the keyworkd or provide a section to define and\n#possible an axis to specify the oritation of the \n#Question:  Best way to specify the section?  Should it be given \n#Error Checks: That the section is within the image\nccddata.subtract_overscan(section='[:,100:110]', function='polynomial', order=3)\n\n#trim the images--the section gives the  part of the image to keep\n#That the trim section is within the image\nccddata.trim_image(section='[0:100,0:100]')\n\n#subtract the master bias.  Although this is a convenience function as subtracting\n#the two arrays will do the same thing.   This should be able to handle logging of\n#of subtracting it off (logging should be added to NDData and then this is really\n#just a convenience function\n#Error checks: the masterbias and image are the same shape\nmasterbias=NDData.NDData(np.zeros(100,100))\nccddata.subtract_bias(masterbias)\n\n#correct for dark frames\n#Error checks: the masterbias and image are the same shape\nmasterdark=NDData.NDData(np.zeros(100,100))\nccddata.subtract_dark(darkframe)\n\n#correct for gain--once again gain should have a unit and even an error associated with it.  \nccddata.gain_correct(gain=1.0)\n#Also the gain may be non-linear\nccddata.gain_correct(gain=np.array([1.0,0.5e-3])\n#although then this step should be apply before any other corrections if it is non-linear\n#but that is more up to the person processing their own data.\n\n#crosstalk corrections--also potential a convenience function, but basically multiples the\n#xtalkimage by the coeffient and then subtracts it.  It is kept general because this will\n#be dependent on the CCD and the set up of the CCD.   Not applicable for a single CCD\n#situation\n#Error checks: the xtalkimage and image are the same shape\nxtalkimage=NDData.NDData(np.zeros(100,100))\nccddata.xtalk_correct(xtalkimage, coef=1e-3)\n\n#flat field correction--this can either be a dome flat, sky flat, or an \n#illumination corrected image.  This step should normalize by the value of the\n#flatfield after dividing by it.\n#Error checks: the  flatimage and image are the same shape\n#Error checks: check for divive by zero\n#Features: If the flat is less than minvalue, minvalue is used\nflatimage=NDData.NDData(np.ones(100,100))\nccddata.flat_correct(flatimage, minvalue=1)\n\n#fringe correction or any correction that requires subtracting\n#off a potentially scaled image\n#Error checks: the  flatimage and image are the same shape\nfringemage=NDData.NDData(np.zero(100,100))\nccddata.fringe_correct(fringeimage, scale=1)\n\n#cosmic ray cleaning step--this should have options for different\n#ways to do it with their associated steps.  We also might want to \n#implement this as a slightly different step.  The cosmic ray cleaning\n#step should update the mask and flags. So the user could have options\n#to replace the cosmic rays, only flag the cosmic rays, or flag and \n#mask the cosmic rays, or all of the above.\nccddata.cosmicray_clean(method='laplace', args=*kwargs)\n\n#Apply distortion corrections\n#Either update the WCS or transform the frame\nccddata.distortion_correct(distortion)\n\n\n# ================\n# Helper Functions \n# ================\n\n#fit a 1-D function with iterative rejections and the ability to \n#select different functions to fit.  \n#other options are reject parameters, number of iteractions \n#and/or convergernce limit\ncoef=iterfit(x, y, function='polynomial', order=3)\n\n#fit a 2-D function with iterative rejections and the ability to \n#select different functions to fit.  \n#other options are reject parameters, number of iteractions \n#and/or convergernce limit\ncoef=iterfit(data, function='polynomial', order=3)\n\n#combine a set of NDData objects\ncombine([ccddata, ccddata2], method='average', reject=None, **kwargs)\n\n#re-sample the data to different binnings (either larger or smaller\nccddata=rebin(ccddata, binning=(2,2))\n\n#tranform the data--ie shift, rotate, etc\n#question--add convience functions for image shifting and rotation?\n#should udpate WCS although that would actually be the prefered method\nccddata=transform(ccddata, transform, conserve_flux=True)\n"]],"start1":0,"start2":0,"length1":0,"length2":7720}]],"length":7720}
{"contributors":[],"silentsave":false,"ts":1353361244077,"patch":[[{"diffs":[[0," objects"],[1,".  Combining objects will need to handle how to\n#combine the error dat and masks as well.  It should have different methods for\n#combining the data (average, median) and for rejection data (None, sigma clip, \n#ccdclip, minmax).  Rejected pixels should be reflected in the final bad pixel\n#maps.   Methods for scaling the data and weighting the data can also be \n#included.   The task should also handle any memories issues that will occur to\n#dealing with large file sizes."],[0,"\ncombine"]],"start1":7294,"start2":7294,"length1":16,"length2":489},{"diffs":[[0,"x=True)\n"],[1,"\n\n"]],"start1":8185,"start2":8185,"length1":8,"length2":10}]],"length":8195,"saved":false}
